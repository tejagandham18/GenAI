# ğŸ§  Retrieval Augmented Generation (RAG) - How This Application Works

#  **Ingestion pipeline**


This document explains how the RAG application works based on the provided ingestion pipeline code.  
The pipeline prepares documents for RAG usage by converting them into embeddings and storing them inside a vector database.

---

# ğŸ“ 1. High-Level RAG Pipeline

The ingestion pipeline implements the first half of a RAG workflow:

```
Documents â†’ Chunks â†’ Embeddings â†’ Vector Store
```

This prepares data so later query stages can perform retrieval.

---

# ğŸ“ 2. Components of the Ingestion Code

The pipeline executes **three main stages**:

```
1. Load Documents
2. Split Documents into Chunks
3. Embed & Store in Vector DB
```

---

# ğŸ“ 3. Step-by-Step Breakdown

## **(1) Load Documents â€“ `load_documents()`**

**Purpose:** Import raw company documents into memory.

### ğŸ”¹ Internal Operations:

- Verify that the `docs` folder exists
- Load all `.txt` files using `DirectoryLoader`
- Wrap each file into LangChain `Document` objects:

```
Document(
  page_content="Tesla is a clean energy company...",
  metadata={"source": "docs/tesla.txt"}
)
```

Metadata allows tracing retrieved chunks back to files.

---

## **(2) Split into Chunks â€“ `split_documents()`**

**Purpose:** Break large documents into manageable text chunks.

LLMs and embeddings cannot operate efficiently on very long text, so documents are split into smaller pieces.

### ğŸ”¹ Internal Behavior:

Given:
```
chunk_size = 1000
chunk_overlap = 0
```

A 3000-character document becomes:

```
chunk 1: 0-999
chunk 2: 1000-1999
chunk 3: 2000-2999
```

Each chunk still carries metadata such as its source file.

---

## **(3) Embedding & Storage â€“ `create_vector_store()`**

This is the core RAG ingestion step.

### **Step A â€” Compute Embeddings**

The model:
```
OpenAIEmbeddings(model="text-embedding-3-small")
```

Converts chunks into numerical vectors (embeddings) representing semantic meaning, e.g.:

```
[0.12, 0.88, 0.02, ...]
```

### **Step B â€” Store in Vector Database (Chroma)**

Chroma stores:

| Item | Purpose |
|---|---|
| Embedding | Semantic search |
| Chunk text | Context for LLM |
| Metadata | Traceability |
| ID | Document indexing |

Storage is persisted to disk under:

```
db/chroma_db/
```

### **Step C â€” Persistence**

Embedding data is saved so ingestion runs only once.

---

# ğŸ“ 4. Vectorstore Reuse on Next Runs

Before new ingestion, code checks:

```
if os.path.exists("db/chroma_db"):
```

If exists:

âœ” Skip reprocessing  
âœ” Load vector store  
âœ” Ready for semantic retrieval immediately

---

# ğŸ“ 5. What This Pipeline Enables

After ingestion, the system can perform:

```
User Query â†’ Vector Search â†’ Relevant Chunks â†’ LLM Answer
```

Example for query:

> "What does Tesla do?"

The RAG engine retrieves relevant chunks instead of letting LLM hallucinate.

---

# ğŸ“ 6. What is Not Covered Yet (Handled in Later Stages)

This ingestion code does **not**:

âŒ Generate answers  
âŒ Retrieve chunks for questions  
âŒ Handle chat history  
âŒ Perform reranking or hybrid search  

These are handled by:

- `2_retrieval_pipeline.py`
- `3_answer_generation.py`
- `4_history_aware_generation.py`

---

# ğŸ“ 7. Final Summary

The ingestion pipeline performs:

```
Raw Documents
      â†“
Loading (metadata)
      â†“
Chunking (segmentation)
      â†“
Embedding (semantic vectors)
      â†“
Vector Store Persistence (Chroma)
```

This prepares all required data for RAG-based semantic Q&A, enterprise search, and chatbot applications.

---

#  **Retrieval pipeline**


This document explains the retrieval stage of a RAG (Retrieval Augmented Generation) system based on the provided code.

The retrieval pipeline is responsible for finding the most relevant information from stored documents before an LLM generates the answer.

---

# ğŸ“ 1. Purpose of Retrieval Stage

Once documents have been ingested into a Vector Database, the retrieval stage does:

```
User Query â†’ Vector Search â†’ Relevant Context
```

Retrieval ensures that responses are based on real knowledge from documents instead of LLM hallucinations.

---

# ğŸ“ 2. Code Overview

Key components used in the code:

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
```

Retrieval pipeline steps:

1. Load VectorDB (Chroma)
2. Embed the user query
3. Perform similarity search
4. Return the top relevant document chunks

---

# ğŸ“ 3. Step-by-Step Breakdown

## **(1) Load Vector Store + Embeddings**

```python
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

db = Chroma(
    persist_directory="db/chroma_db",
    embedding_function=embedding_model,
    collection_metadata={"hnsw:space": "cosine"}
)
```

### ğŸ§  Internal Behavior:
- Loads stored document embeddings from disk
- Loads HNSW similarity index
- Sets cosine similarity as search metric

This allows the system to search documents efficiently.

---

## **(2) Prepare Query**

```python
query = "How much did Microsoft pay to acquire GitHub?"
```

This is the natural language question from the user.

---

## **(3) Execute Similarity Search**

```python
retriever = db.as_retriever(search_kwargs={"k": 5})
relevant_docs = retriever.invoke(query)
```

This step finds the most similar chunks.

### ğŸ§  Internal Breakdown:

#### **Step A â€” Query Embedding**
The query is converted into a vector:

```
[0.12, 0.56, 0.88, ...]
```

#### **Step B â€” Compare Against Stored Embeddings**
Chroma computes cosine similarity between:

```
Query Vector â†” Document Chunk Vectors
```

#### **Step C â€” Rank & Select**
Returns top `k=5` chunks with highest similarity scores.

This is the **retrieval** part of RAG.

---

## **Optional Threshold Filtering**

```python
retriever = db.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 5, "score_threshold": 0.3}
)
```

This filters out irrelevant chunks to reduce hallucination.

---

## **(4) Display Retrieved Context**

```python
for doc in relevant_docs:
    print(doc.page_content)
```

This prints the text content of retrieved chunks.  
These chunks are called **context**.

---

# ğŸ“ 4. Why Retrieval Matters in RAG

Retrieval ensures:

âœ” factual answers  
âœ” evidence-based responses  
âœ” prevents hallucination  
âœ” supports private knowledge

Without retrieval:

LLM guesses and may answer incorrectly.

With retrieval:

LLM uses verified context from documents.

---

# ğŸ“ 5. Retrieval in Full RAG Workflow

Retrieval sits between ingestion and generation:

```
(1) Ingestion
      â†“
(2) Retrieval â† (this code)
      â†“
(3) Generation (LLM final answer)
```

Retrieval provides the LLM with real context for answering questions.

---

# ğŸ“ 6. Final Summary (Simple)

â¡ **Execute Similarity Search** = find document chunks related to query  
â¡ **Display Context** = show those chunks for LLM usage

Together:

```
User Query â†’ Retrieved Context â†’ LLM â†’ Final Answer
```

This completes the retrieval stage of RAG.

---
