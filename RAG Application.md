# ğŸ§  Retrieval Augmented Generation (RAG) - How This Application Works

This document explains how the RAG application works based on the provided ingestion pipeline code.  
The pipeline prepares documents for RAG usage by converting them into embeddings and storing them inside a vector database.

---

# ğŸ“ 1. High-Level RAG Pipeline

The ingestion pipeline implements the first half of a RAG workflow:

```
Documents â†’ Chunks â†’ Embeddings â†’ Vector Store
```

This prepares data so later query stages can perform retrieval.

---

# ğŸ“ 2. Components of the Ingestion Code

The pipeline executes **three main stages**:

```
1. Load Documents
2. Split Documents into Chunks
3. Embed & Store in Vector DB
```

---

# ğŸ“ 3. Step-by-Step Breakdown

## **(1) Load Documents â€“ `load_documents()`**

**Purpose:** Import raw company documents into memory.

### ğŸ”¹ Internal Operations:

- Verify that the `docs` folder exists
- Load all `.txt` files using `DirectoryLoader`
- Wrap each file into LangChain `Document` objects:

```
Document(
  page_content="Tesla is a clean energy company...",
  metadata={"source": "docs/tesla.txt"}
)
```

Metadata allows tracing retrieved chunks back to files.

---

## **(2) Split into Chunks â€“ `split_documents()`**

**Purpose:** Break large documents into manageable text chunks.

LLMs and embeddings cannot operate efficiently on very long text, so documents are split into smaller pieces.

### ğŸ”¹ Internal Behavior:

Given:
```
chunk_size = 1000
chunk_overlap = 0
```

A 3000-character document becomes:

```
chunk 1: 0-999
chunk 2: 1000-1999
chunk 3: 2000-2999
```

Each chunk still carries metadata such as its source file.

---

## **(3) Embedding & Storage â€“ `create_vector_store()`**

This is the core RAG ingestion step.

### **Step A â€” Compute Embeddings**

The model:
```
OpenAIEmbeddings(model="text-embedding-3-small")
```

Converts chunks into numerical vectors (embeddings) representing semantic meaning, e.g.:

```
[0.12, 0.88, 0.02, ...]
```

### **Step B â€” Store in Vector Database (Chroma)**

Chroma stores:

| Item | Purpose |
|---|---|
| Embedding | Semantic search |
| Chunk text | Context for LLM |
| Metadata | Traceability |
| ID | Document indexing |

Storage is persisted to disk under:

```
db/chroma_db/
```

### **Step C â€” Persistence**

Embedding data is saved so ingestion runs only once.

---

# ğŸ“ 4. Vectorstore Reuse on Next Runs

Before new ingestion, code checks:

```
if os.path.exists("db/chroma_db"):
```

If exists:

âœ” Skip reprocessing  
âœ” Load vector store  
âœ” Ready for semantic retrieval immediately

---

# ğŸ“ 5. What This Pipeline Enables

After ingestion, the system can perform:

```
User Query â†’ Vector Search â†’ Relevant Chunks â†’ LLM Answer
```

Example for query:

> "What does Tesla do?"

The RAG engine retrieves relevant chunks instead of letting LLM hallucinate.

---

# ğŸ“ 6. What is Not Covered Yet (Handled in Later Stages)

This ingestion code does **not**:

âŒ Generate answers  
âŒ Retrieve chunks for questions  
âŒ Handle chat history  
âŒ Perform reranking or hybrid search  

These are handled by:

- `2_retrieval_pipeline.py`
- `3_answer_generation.py`
- `4_history_aware_generation.py`

---

# ğŸ“ 7. Final Summary

The ingestion pipeline performs:

```
Raw Documents
      â†“
Loading (metadata)
      â†“
Chunking (segmentation)
      â†“
Embedding (semantic vectors)
      â†“
Vector Store Persistence (Chroma)
```

This prepares all required data for RAG-based semantic Q&A, enterprise search, and chatbot applications.

---

